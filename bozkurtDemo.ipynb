{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from extras import foldGeneration\n",
    "import numpy as np\n",
    "from ModeTonicEstimation import BozkurtEstimation\n",
    "from ModeTonicEstimation import Evaluator as ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have out data set in \"./data\". Her klasörün içinde o makama ait kayitlarin ppitch trackleri var. (copyright issuelari dolayisiyla kayitlari veremiyoruz.) Bunu hesaplamak icin extras'taki kodu kullandik. Ama ayni formatta kaydetmek kaydiyla (tek sutunda pitch degerleri), istediginiz melody extraction algoritmasini kullanabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_dir = 'demo'\n",
    "data_dir = os.path.join(demo_dir, 'data')\n",
    "annotation_file = os.path.join(data_dir, 'annotations.json')\n",
    "\n",
    "be = BozkurtEstimation.BozkurtEstimation(step_size=7.5, smooth_factor=15, chunk_size=0, frame_rate=128/44100)\n",
    "evaluator = ev.Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datayi foldlara ayiriyoruz, ki denemelerimiz data dependent olsun. demoda az sayida kayit oldugu icin 5 fold kullaniyoruz. Isterseniz makam_recognition_test_dataset'in tamamini indirip bunu deneyebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as a:\n",
    "    annotation = json.load(a)\n",
    "modes = set([m['makam'] for m in annotation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "folds = foldGeneration.stratified_fold(data_dir, annotation_file, n_folds)\n",
    "\n",
    "# we will use the first fold only\n",
    "fold = folds['fold0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mbids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for cur_mode in modes:\n",
    "    print \"Training: \" + cur_mode\n",
    "    [file_list, tonic_list] = zip(*[(rec['file'], rec['tonic']) for rec in fold['train'] \n",
    "                                    if rec['mode'] == cur_mode])\n",
    "    models[cur_mode] = be.train(cur_mode, file_list, tonic_list, metric='pcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for cur_mode in modes:\n",
    "    ax.plot(models[cur_mode].bins, models[cur_mode].vals, label=cur_mode)\n",
    "\n",
    "legend = ax.legend(loc='upper right')    \n",
    "    \n",
    "plt.xlabel('Normalized Pitch Class (cents)')\n",
    "plt.title('Pitch Class Distributions of the Makams')\n",
    "plt.grid(True)\n",
    "    \n",
    "ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "From the distributions you can see some important differences in the peak locations, heights and shapes. In broder sense, these aspects of the distributions convey information about how the notes are being performed. For instance the 3rd degree of Hicaz makam (i.e. the third peak) does not coincide with the 3rd degrees of Saba and Hüseyni. Similarly the third degree of Saba makam is performed more frequent than the third of Hüseyni makam.\n",
    "\n",
    "Using these distributions for comparison, we can find out the makam and/or tonic of another recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the pitch tracks for all the testing recordings. This is needed for all the estimation tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with tonic identification. Here, the assumption is given a recording with known makam and unknown tonic, the peaks of the makam distribution and the peaks of the distribution of the recording should coincide. We can check this by shifting the distribution of the template and finding the best shift according to a distance/similarity metric. The best shift will simply give us the tonic frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tonic Identification\n",
    "for rec in fold['test']:\n",
    "    res = be.estimate(rec['file'], mode_in=models[rec['mode']], distance_method=\"bhat\")[0][0]\n",
    "    \n",
    "    # evaluate\n",
    "    e = evaluator.tonic_evaluate(rec['mbid'], res, rec['tonic'])\n",
    "    \n",
    "    print(rec['mbid'] + \", Anno: \" + (\"%.1f\" % rec['tonic']) + \" Hz, \"\n",
    "          \"Est: \" + (\"%.1f\" % res) + ' Hz: ' + str(e['tonic_eval']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for these 3 recordings, we identified all the tonics correctly! Note that this is a rather small and easy experiment (only 3 recordings, 3 classes etc.), so don't expect a \"perfect\" result for real datasets. \n",
    "\n",
    "Also, you might have noticed that the estimated tonic frequencies have octave errors. This is because of the octave we restrict ourselves when computing the PCDs. In this case by computing the \"trend\" of the pitch track, one can easily shift the estimated tonic to the correct octave. In practice, we cannot argue a definite tonic octace in many makam performances (esp. instrumental pieces), because all the instruments perform the same (heterophonic) melody in their own register; i.e. each performer has their own tonic frequency, all of which belong to the same pitch class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with makam recognition. If we know the tonic of the recording in advance, we can compare the distribution extracted from the recording with the makam distributions. The closest/most-similar distribution would point the estimated makam.\n",
    "\n",
    "Tonic identification icin artik bundan daha iyi metodlar var. Hint muzikleri icin Gulati, makamlar icin Sercan. Bu sebeple histogram metodunun en kullanisli oldugu alan artik makam recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Makam Recognition\n",
    "for rec in fold['test']:\n",
    "    res = be.estimate(rec['file'], mode_in=models, tonic_freq=rec['tonic'],  # first zero is to get the first rank\n",
    "                                        distance_method=\"bhat\")[0][0]         # second is to get the label\n",
    "    \n",
    "    # evaluate\n",
    "    e = evaluator.mode_evaluate(rec['mbid'], res, rec['mode'])\n",
    "    \n",
    "    print(rec['mbid'] + \", Anno: \" + rec['mode'] + \", \"\n",
    "          \"Est: \" + res + ', ' + str(e['mode_eval']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, everything is correct! Apart from the fact that the dataset is small, these three makams are very distinct from each other as explained earlier. You should expect substantial confusion between similar makams (esp. transposed and compound types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, joint estimation of makam and tonic... Here we know neither the tonic nor the makam of the recording. In this case we basically check every (tonic-candidate, makam) combination. The closest (or the most similar) comparison provides us both the estimated tonic frequency and the makam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Joint Tonic and Makam Estimation\n",
    "for rec in fold['test']:\n",
    "    res_mode, res_tonic = be.estimate(rec['file'], mode_in=models, distance_method=\"bhat\")\n",
    "    res_mode = res_mode[0][0]\n",
    "    res_tonic = res_tonic[0][0]\n",
    "    \n",
    "    # evaluate\n",
    "    e = evaluator.joint_evaluate(rec['mbid'], [res_tonic, rec['tonic']], [res_mode, rec['mode']])\n",
    "    \n",
    "    print(rec['mbid'] + \", Anno: \" + rec['mode'] + \", \"\n",
    "              \"Est: \" + res_mode + ', ' + str(e['mode_eval']))\n",
    "    print(\" \" * len(rec['mbid'] + \", \") + \"Anno: \" + (\"%.1f\" % rec['tonic']) + \" Hz, \"\n",
    "              \"Est: \" + (\"%.1f\" % res_tonic) + ' Hz: ' + str(e['tonic_eval']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again a perfect result! Note that in a meaningful dataset, this last step will not be so accurate as many makams tend to differ from each other with the location of a single interval, the relative strength (how much they are played compared to others) of some intervals etc. \n",
    "\n",
    "Buraya diger dezavantajlari yaz,"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
